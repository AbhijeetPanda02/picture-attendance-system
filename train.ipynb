{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67b3f755-0b36-44a8-976f-6974301cc25d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet-pytorch in c:\\users\\91977\\anaconda3\\lib\\site-packages (2.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: mtcnn in c:\\users\\91977\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\91977\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy in c:\\users\\91977\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91977\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from facenet-pytorch) (10.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from facenet-pytorch) (2.32.2)\n",
      "Requirement already satisfied: torch<2.3.0,>=2.2.0 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from facenet-pytorch) (2.2.2)\n",
      "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from facenet-pytorch) (0.17.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from facenet-pytorch) (4.66.4)\n",
      "Requirement already satisfied: joblib>=1.4.2 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from mtcnn) (1.4.2)\n",
      "Requirement already satisfied: lz4>=4.3.3 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from mtcnn) (4.4.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.4.26)\n",
      "Requirement already satisfied: filelock in c:\\users\\91977\\anaconda3\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\91977\\anaconda3\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\91977\\anaconda3\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\91977\\anaconda3\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2024.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\91977\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.0.0->facenet-pytorch) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\91977\\anaconda3\\lib\\site-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install facenet-pytorch mtcnn opencv-python numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd05a34d-8c0f-4a0b-aefd-59d34c595bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No face detected in Dataset\\2241013015\\72.jpg\n",
      "❌ No face detected in Dataset\\2241013015\\76.jpg\n",
      "❌ No face detected in Dataset\\2241013015\\77.jpg\n",
      "❌ No face detected in Dataset\\2241013015\\78.jpg\n",
      "❌ No face detected in Dataset\\2241013015\\79.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [01:12<00:39, 19.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No face detected in Dataset\\2241016078\\77.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:21<00:00, 23.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings saved successfully as student_embeddings.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "import torch\n",
    "\n",
    "# Initialize MTCNN (face detector) and InceptionResnetV1 (embedding model)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "detector = MTCNN(image_size=160, margin=20, device=device)\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Dataset\n",
    "dataset_path = \"Dataset\"\n",
    "embeddings_dict = {}\n",
    "\n",
    "# Extracting the embeddings\n",
    "def extract_embedding(img_path):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        face = detector(img)\n",
    "        if face is not None:\n",
    "            face = face.unsqueeze(0).to(device)\n",
    "            emb = model(face)\n",
    "            return emb.detach().cpu().numpy()[0]\n",
    "        else:\n",
    "            print(f\"❌ No face detected in {img_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Loop through each student's folder\n",
    "for student_id in tqdm(os.listdir(dataset_path)):\n",
    "    student_folder = os.path.join(dataset_path, student_id)\n",
    "\n",
    "    # Skip is required\n",
    "    if not os.path.isdir(student_folder) or student_id.startswith(\".\"):\n",
    "        continue\n",
    "\n",
    "    embeddings = []\n",
    "    for img_name in os.listdir(student_folder):\n",
    "        if img_name.startswith('.'):  # Skip hidden/system files\n",
    "            continue\n",
    "        img_path = os.path.join(student_folder, img_name)\n",
    "        emb = extract_embedding(img_path)\n",
    "        if emb is not None:\n",
    "            embeddings.append(emb)\n",
    "\n",
    "    if embeddings:\n",
    "        avg_embedding = np.mean(embeddings, axis=0)\n",
    "        embeddings_dict[student_id] = avg_embedding\n",
    "    else:\n",
    "        print(f\"⚠️ No valid images found for {student_id}\")\n",
    "\n",
    "# Save the embeddings\n",
    "np.save(\"student_embeddings_02.npy\", embeddings_dict)\n",
    "print(\"✅ Embeddings saved successfully as student_embeddings.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ce010f2-4be0-4680-a9e1-53c90e566872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Detected 6 face(s).\n",
      "🧪 Comparing with 2241013015: distance = 1.2025\n",
      "🧪 Comparing with 2241016042: distance = 1.0022\n",
      "⚠️ No match for face #1\n",
      "🧪 Comparing with 2241013015: distance = 1.0809\n",
      "🧪 Comparing with 2241016042: distance = 1.2182\n",
      "⚠️ No match for face #2\n",
      "🧪 Comparing with 2241013015: distance = 0.9780\n",
      "🧪 Comparing with 2241016042: distance = 1.0970\n",
      "⚠️ No match for face #3\n",
      "🧪 Comparing with 2241013015: distance = 1.0948\n",
      "🧪 Comparing with 2241016042: distance = 0.6728\n",
      "✅ Match found: 2241016042\n",
      "🧪 Comparing with 2241013015: distance = 0.7581\n",
      "✅ Match found: 2241013015\n",
      "🧪 Comparing with 2241013015: distance = 1.2917\n",
      "🧪 Comparing with 2241016042: distance = 1.2635\n",
      "⚠️ No match for face #6\n",
      "\n",
      "✅ Present Students:\n",
      "['2241016042', '2241013015']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import os\n",
    "\n",
    "# Load model and embeddings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mtcnn = MTCNN(image_size=160, margin=0, keep_all=True, device=device)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Student embeddings loading\n",
    "student_data = np.load(\"student_embeddings.npy\", allow_pickle=True)\n",
    "student_embeddings = student_data.item()\n",
    "\n",
    "# Group image loading\n",
    "group_img_path = \"WhatsApp Image 2025-08-02 at 00.06.04_b696b984.jpg\"  # replace with your image path\n",
    "img = Image.open(group_img_path).convert('RGB')\n",
    "\n",
    "# Detect faces\n",
    "boxes, probs = mtcnn.detect(img)\n",
    "\n",
    "if boxes is None:\n",
    "    \n",
    "    print(\"❌ No faces detected.\")\n",
    "else:\n",
    "    print(f\"✅ Detected {len(boxes)} face(s).\")\n",
    "\n",
    "present_students = []\n",
    "\n",
    "# Drawing object\n",
    "draw = ImageDraw.Draw(img)\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "# Processing each detected face\n",
    "for i, box in enumerate(boxes):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    face_crop = img.crop((x1, y1, x2, y2)).resize((160, 160))\n",
    "    face_tensor = torch.tensor(np.array(face_crop)).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = resnet(face_tensor)\n",
    "\n",
    "    # Comparing with all stored embeddings\n",
    "    matched = False\n",
    "    for reg_num, emb in student_embeddings.items():\n",
    "        emb_tensor = torch.tensor(emb).unsqueeze(0).to(device).float()\n",
    "        dist = torch.norm(embedding - emb_tensor).item()\n",
    "        print(f\"🧪 Comparing with {reg_num}: distance = {dist:.4f}\")\n",
    "\n",
    "        if dist < 0.9:  # Adjust threshold if needed\n",
    "            print(f\"✅ Match found: {reg_num}\")\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"green\", width=2)\n",
    "            draw.text((x1, y2 + 2), reg_num, fill=\"green\", font=font)\n",
    "            present_students.append(reg_num)\n",
    "            matched = True\n",
    "            break\n",
    "\n",
    "    if not matched:\n",
    "        print(f\"⚠️ No match for face #{i + 1}\")\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y2 + 2), \"Unknown\", fill=\"red\", font=font)\n",
    "\n",
    "\n",
    "img.save(\"output_with_boxes.jpg\")\n",
    "img.show()\n",
    "\n",
    "print(\"\\n✅ Present Students:\")\n",
    "print(list(set(present_students)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62da7f19-0b3e-4bba-a460-4e78ee682c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Detected 6 face(s).\n",
      "🧪 Comparing with 2241013015: distance = 1.1444\n",
      "🧪 Comparing with 2241016042: distance = 1.1129\n",
      "⚠️ No match for face #1\n",
      "🧪 Comparing with 2241013015: distance = 1.1769\n",
      "🧪 Comparing with 2241016042: distance = 1.2346\n",
      "⚠️ No match for face #2\n",
      "🧪 Comparing with 2241013015: distance = 0.6309\n",
      "✅ Match found: 2241013015\n",
      "🧪 Comparing with 2241013015: distance = 1.1101\n",
      "🧪 Comparing with 2241016042: distance = 1.3723\n",
      "⚠️ No match for face #4\n",
      "🧪 Comparing with 2241013015: distance = 1.0702\n",
      "🧪 Comparing with 2241016042: distance = 0.5295\n",
      "✅ Match found: 2241016042\n",
      "🧪 Comparing with 2241013015: distance = 0.8799\n",
      "✅ Match found: 2241013015\n",
      "\n",
      "✅ Present Students:\n",
      "['2241016042', '2241013015']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import os\n",
    "\n",
    "# Load model and embeddings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mtcnn = MTCNN(image_size=160, margin=0, keep_all=True, device=device)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Student embeddings loading\n",
    "student_data = np.load(\"student_embeddings.npy\", allow_pickle=True)\n",
    "student_embeddings = student_data.item()\n",
    "\n",
    "# Group image loading\n",
    "group_img_path = \"WhatsApp Image 2025-08-02 at 00.06.05_dc1245d5.jpg\"  # replace with your image path\n",
    "img = Image.open(group_img_path).convert('RGB')\n",
    "\n",
    "# Detect faces\n",
    "boxes, probs = mtcnn.detect(img)\n",
    "\n",
    "if boxes is None:\n",
    "    print(\"❌ No faces detected.\")\n",
    "else:\n",
    "    print(f\"✅ Detected {len(boxes)} face(s).\")\n",
    "\n",
    "present_students = []\n",
    "\n",
    "# Drawing object\n",
    "draw = ImageDraw.Draw(img)\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "# Processing each detected face\n",
    "for i, box in enumerate(boxes):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    face_crop = img.crop((x1, y1, x2, y2)).resize((160, 160))\n",
    "    face_tensor = torch.tensor(np.array(face_crop)).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = resnet(face_tensor)\n",
    "\n",
    "    # Comparing with all stored embeddings\n",
    "    matched = False\n",
    "    for reg_num, emb in student_embeddings.items():\n",
    "        emb_tensor = torch.tensor(emb).unsqueeze(0).to(device).float()\n",
    "        dist = torch.norm(embedding - emb_tensor).item()\n",
    "        print(f\"🧪 Comparing with {reg_num}: distance = {dist:.4f}\")\n",
    "\n",
    "        if dist < 0.9:  # Adjust threshold if needed\n",
    "            print(f\"✅ Match found: {reg_num}\")\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"green\", width=2)\n",
    "            draw.text((x1, y2 + 2), reg_num, fill=\"green\", font=font)\n",
    "            present_students.append(reg_num)\n",
    "            matched = True\n",
    "            break\n",
    "\n",
    "    if not matched:\n",
    "        print(f\"⚠️ No match for face #{i + 1}\")\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y2 + 2), \"Unknown\", fill=\"red\", font=font)\n",
    "\n",
    "\n",
    "img.save(\"output_with_boxes.jpg\")\n",
    "img.show()\n",
    "\n",
    "print(\"\\n✅ Present Students:\")\n",
    "print(list(set(present_students)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8e6dab3-21da-4d2b-b691-6c7621dbf60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Detected 2 face(s).\n",
      "🧪 Comparing with 2241013015: distance = 0.9143\n",
      "🧪 Comparing with 2241016042: distance = 1.0670\n",
      "⚠️ No match for face #1\n",
      "🧪 Comparing with 2241013015: distance = 1.1122\n",
      "🧪 Comparing with 2241016042: distance = 0.6976\n",
      "✅ Match found: 2241016042\n",
      "\n",
      "✅ Present Students:\n",
      "['2241016042']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import os\n",
    "\n",
    "# Load model and embeddings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mtcnn = MTCNN(image_size=160, margin=0, keep_all=True, device=device)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Student embeddings loading\n",
    "student_data = np.load(\"student_embeddings.npy\", allow_pickle=True)\n",
    "student_embeddings = student_data.item()\n",
    "\n",
    "# Group image loading\n",
    "group_img_path = \"WhatsApp Image 2025-08-02 at 00.13.27_6693aeba.jpg\"  # replace with your image path\n",
    "img = Image.open(group_img_path).convert('RGB')\n",
    "\n",
    "# Detect faces\n",
    "boxes, probs = mtcnn.detect(img)\n",
    "\n",
    "if boxes is None:\n",
    "    print(\"❌ No faces detected.\")\n",
    "else:\n",
    "    print(f\"✅ Detected {len(boxes)} face(s).\")\n",
    "\n",
    "present_students = []\n",
    "\n",
    "# Drawing object\n",
    "draw = ImageDraw.Draw(img)\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "# Processing each detected face\n",
    "for i, box in enumerate(boxes):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    face_crop = img.crop((x1, y1, x2, y2)).resize((160, 160))\n",
    "    face_tensor = torch.tensor(np.array(face_crop)).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = resnet(face_tensor)\n",
    "\n",
    "    # Comparing with all stored embeddings\n",
    "    matched = False\n",
    "    for reg_num, emb in student_embeddings.items():\n",
    "        emb_tensor = torch.tensor(emb).unsqueeze(0).to(device).float()\n",
    "        dist = torch.norm(embedding - emb_tensor).item()\n",
    "        print(f\"🧪 Comparing with {reg_num}: distance = {dist:.4f}\")\n",
    "\n",
    "        if dist < 0.9:  # Adjust threshold if needed\n",
    "            print(f\"✅ Match found: {reg_num}\")\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"green\", width=2)\n",
    "            draw.text((x1, y2 + 2), reg_num, fill=\"green\", font=font)\n",
    "            present_students.append(reg_num)\n",
    "            matched = True\n",
    "            break\n",
    "\n",
    "    if not matched:\n",
    "        print(f\"⚠️ No match for face #{i + 1}\")\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y2 + 2), \"Unknown\", fill=\"red\", font=font)\n",
    "\n",
    "\n",
    "img.save(\"output_with_boxes.jpg\")\n",
    "img.show()\n",
    "\n",
    "print(\"\\n✅ Present Students:\")\n",
    "print(list(set(present_students)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c71d435e-06b3-4604-8100-b59f52d68ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Detected 21 face(s).\n",
      "🧪 Comparing with 2241013015: distance = 1.0787\n",
      "🧪 Comparing with 22410160124: distance = 1.1343\n",
      "🧪 Comparing with 2241016042: distance = 0.5991\n",
      "✅ Match found: 2241016042\n",
      "🧪 Comparing with 2241013015: distance = 1.2990\n",
      "🧪 Comparing with 22410160124: distance = 1.1951\n",
      "🧪 Comparing with 2241016042: distance = 0.9828\n",
      "🧪 Comparing with 2241016056: distance = 1.0470\n",
      "🧪 Comparing with 2241016078: distance = 1.1022\n",
      "🧪 Comparing with 2241016088: distance = 0.8192\n",
      "✅ Match found: 2241016088\n",
      "🧪 Comparing with 2241013015: distance = 1.1686\n",
      "🧪 Comparing with 22410160124: distance = 1.0066\n",
      "🧪 Comparing with 2241016042: distance = 1.0131\n",
      "🧪 Comparing with 2241016056: distance = 1.1401\n",
      "🧪 Comparing with 2241016078: distance = 0.8659\n",
      "✅ Match found: 2241016078\n",
      "🧪 Comparing with 2241013015: distance = 1.2133\n",
      "🧪 Comparing with 22410160124: distance = 0.9282\n",
      "🧪 Comparing with 2241016042: distance = 0.9247\n",
      "🧪 Comparing with 2241016056: distance = 0.8729\n",
      "✅ Match found: 2241016056\n",
      "🧪 Comparing with 2241013015: distance = 1.2401\n",
      "🧪 Comparing with 22410160124: distance = 1.2330\n",
      "🧪 Comparing with 2241016042: distance = 1.1861\n",
      "🧪 Comparing with 2241016056: distance = 1.1597\n",
      "🧪 Comparing with 2241016078: distance = 0.9582\n",
      "🧪 Comparing with 2241016088: distance = 1.1470\n",
      "⚠️ No match for face #5\n",
      "🧪 Comparing with 2241013015: distance = 1.1488\n",
      "🧪 Comparing with 22410160124: distance = 1.1926\n",
      "🧪 Comparing with 2241016042: distance = 1.1169\n",
      "🧪 Comparing with 2241016056: distance = 1.1611\n",
      "🧪 Comparing with 2241016078: distance = 0.6135\n",
      "✅ Match found: 2241016078\n",
      "🧪 Comparing with 2241013015: distance = 1.0548\n",
      "🧪 Comparing with 22410160124: distance = 1.1759\n",
      "🧪 Comparing with 2241016042: distance = 1.0863\n",
      "🧪 Comparing with 2241016056: distance = 0.9747\n",
      "🧪 Comparing with 2241016078: distance = 1.0874\n",
      "🧪 Comparing with 2241016088: distance = 1.2650\n",
      "⚠️ No match for face #7\n",
      "🧪 Comparing with 2241013015: distance = 1.1329\n",
      "🧪 Comparing with 22410160124: distance = 1.1340\n",
      "🧪 Comparing with 2241016042: distance = 0.8866\n",
      "✅ Match found: 2241016042\n",
      "🧪 Comparing with 2241013015: distance = 1.0970\n",
      "🧪 Comparing with 22410160124: distance = 1.0727\n",
      "🧪 Comparing with 2241016042: distance = 0.9399\n",
      "🧪 Comparing with 2241016056: distance = 1.1519\n",
      "🧪 Comparing with 2241016078: distance = 1.0219\n",
      "🧪 Comparing with 2241016088: distance = 1.0618\n",
      "⚠️ No match for face #9\n",
      "🧪 Comparing with 2241013015: distance = 1.2486\n",
      "🧪 Comparing with 22410160124: distance = 1.3408\n",
      "🧪 Comparing with 2241016042: distance = 1.1419\n",
      "🧪 Comparing with 2241016056: distance = 1.1305\n",
      "🧪 Comparing with 2241016078: distance = 1.2814\n",
      "🧪 Comparing with 2241016088: distance = 1.2430\n",
      "⚠️ No match for face #10\n",
      "🧪 Comparing with 2241013015: distance = 1.1591\n",
      "🧪 Comparing with 22410160124: distance = 0.9590\n",
      "🧪 Comparing with 2241016042: distance = 1.0137\n",
      "🧪 Comparing with 2241016056: distance = 0.9164\n",
      "🧪 Comparing with 2241016078: distance = 0.9790\n",
      "🧪 Comparing with 2241016088: distance = 1.0224\n",
      "⚠️ No match for face #11\n",
      "🧪 Comparing with 2241013015: distance = 1.1939\n",
      "🧪 Comparing with 22410160124: distance = 1.2400\n",
      "🧪 Comparing with 2241016042: distance = 0.9358\n",
      "🧪 Comparing with 2241016056: distance = 1.2378\n",
      "🧪 Comparing with 2241016078: distance = 0.9876\n",
      "🧪 Comparing with 2241016088: distance = 1.0032\n",
      "⚠️ No match for face #12\n",
      "🧪 Comparing with 2241013015: distance = 1.2658\n",
      "🧪 Comparing with 22410160124: distance = 1.0407\n",
      "🧪 Comparing with 2241016042: distance = 0.9590\n",
      "🧪 Comparing with 2241016056: distance = 1.0622\n",
      "🧪 Comparing with 2241016078: distance = 1.1590\n",
      "🧪 Comparing with 2241016088: distance = 1.0377\n",
      "⚠️ No match for face #13\n",
      "🧪 Comparing with 2241013015: distance = 1.1485\n",
      "🧪 Comparing with 22410160124: distance = 0.6341\n",
      "✅ Match found: 22410160124\n",
      "🧪 Comparing with 2241013015: distance = 1.2585\n",
      "🧪 Comparing with 22410160124: distance = 1.0959\n",
      "🧪 Comparing with 2241016042: distance = 1.0689\n",
      "🧪 Comparing with 2241016056: distance = 0.9990\n",
      "🧪 Comparing with 2241016078: distance = 0.9037\n",
      "🧪 Comparing with 2241016088: distance = 0.5239\n",
      "✅ Match found: 2241016088\n",
      "🧪 Comparing with 2241013015: distance = 1.1925\n",
      "🧪 Comparing with 22410160124: distance = 1.1065\n",
      "🧪 Comparing with 2241016042: distance = 1.1744\n",
      "🧪 Comparing with 2241016056: distance = 0.4923\n",
      "✅ Match found: 2241016056\n",
      "🧪 Comparing with 2241013015: distance = 1.2186\n",
      "🧪 Comparing with 22410160124: distance = 1.1614\n",
      "🧪 Comparing with 2241016042: distance = 1.0995\n",
      "🧪 Comparing with 2241016056: distance = 1.0072\n",
      "🧪 Comparing with 2241016078: distance = 0.9983\n",
      "🧪 Comparing with 2241016088: distance = 1.2206\n",
      "⚠️ No match for face #17\n",
      "🧪 Comparing with 2241013015: distance = 1.2644\n",
      "🧪 Comparing with 22410160124: distance = 1.1031\n",
      "🧪 Comparing with 2241016042: distance = 1.2580\n",
      "🧪 Comparing with 2241016056: distance = 1.1302\n",
      "🧪 Comparing with 2241016078: distance = 1.2191\n",
      "🧪 Comparing with 2241016088: distance = 1.0594\n",
      "⚠️ No match for face #18\n",
      "🧪 Comparing with 2241013015: distance = 1.2896\n",
      "🧪 Comparing with 22410160124: distance = 1.2214\n",
      "🧪 Comparing with 2241016042: distance = 1.1607\n",
      "🧪 Comparing with 2241016056: distance = 0.9719\n",
      "🧪 Comparing with 2241016078: distance = 1.2048\n",
      "🧪 Comparing with 2241016088: distance = 1.1498\n",
      "⚠️ No match for face #19\n",
      "🧪 Comparing with 2241013015: distance = 1.1512\n",
      "🧪 Comparing with 22410160124: distance = 1.0524\n",
      "🧪 Comparing with 2241016042: distance = 1.0163\n",
      "🧪 Comparing with 2241016056: distance = 1.0835\n",
      "🧪 Comparing with 2241016078: distance = 1.0952\n",
      "🧪 Comparing with 2241016088: distance = 1.1584\n",
      "⚠️ No match for face #20\n",
      "🧪 Comparing with 2241013015: distance = 1.0500\n",
      "🧪 Comparing with 22410160124: distance = 1.1600\n",
      "🧪 Comparing with 2241016042: distance = 0.9211\n",
      "🧪 Comparing with 2241016056: distance = 0.9668\n",
      "🧪 Comparing with 2241016078: distance = 0.9868\n",
      "🧪 Comparing with 2241016088: distance = 0.9946\n",
      "⚠️ No match for face #21\n",
      "\n",
      "✅ Present Students:\n",
      "['2241016056', '2241016042', '2241016088', '22410160124', '2241016078']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import os\n",
    "\n",
    "# Load model and embeddings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mtcnn = MTCNN(image_size=160, margin=0, keep_all=True, device=device)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Student embeddings loading\n",
    "student_data = np.load(\"student_embeddings_02.npy\", allow_pickle=True)\n",
    "student_embeddings = student_data.item()\n",
    "\n",
    "# Group image loading\n",
    "group_img_path = \"picnic01.jpg\"  # replace with your image path\n",
    "img = Image.open(group_img_path).convert('RGB')\n",
    "\n",
    "# Detect faces\n",
    "boxes, probs = mtcnn.detect(img)\n",
    "\n",
    "if boxes is None:\n",
    "    print(\"❌ No faces detected.\")\n",
    "else:\n",
    "    print(f\"✅ Detected {len(boxes)} face(s).\")\n",
    "\n",
    "present_students = []\n",
    "\n",
    "# Drawing object\n",
    "draw = ImageDraw.Draw(img)\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "# Processing each detected face\n",
    "for i, box in enumerate(boxes):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    face_crop = img.crop((x1, y1, x2, y2)).resize((160, 160))\n",
    "    face_tensor = torch.tensor(np.array(face_crop)).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = resnet(face_tensor)\n",
    "\n",
    "    # Comparing with all stored embeddings\n",
    "    matched = False\n",
    "    for reg_num, emb in student_embeddings.items():\n",
    "        emb_tensor = torch.tensor(emb).unsqueeze(0).to(device).float()\n",
    "        dist = torch.norm(embedding - emb_tensor).item()\n",
    "        print(f\"🧪 Comparing with {reg_num}: distance = {dist:.4f}\")\n",
    "\n",
    "        if dist < 0.9:  # Adjust threshold if needed\n",
    "            print(f\"✅ Match found: {reg_num}\")\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"green\", width=2)\n",
    "            draw.text((x1, y2 + 2), reg_num, fill=\"green\", font=font)\n",
    "            present_students.append(reg_num)\n",
    "            matched = True\n",
    "            break\n",
    "\n",
    "    if not matched:\n",
    "        print(f\"⚠️ No match for face #{i + 1}\")\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y2 + 2), \"Unknown\", fill=\"red\", font=font)\n",
    "\n",
    "\n",
    "img.save(\"output_with_boxes.jpg\")\n",
    "img.show()\n",
    "\n",
    "print(\"\\n✅ Present Students:\")\n",
    "print(list(set(present_students)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c74f60-6e31-403c-9400-80d424993e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8c074a-b86a-472b-bc8a-cd1a2e82b52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
